{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianleguo/.pyenv/versions/3.8.18/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using shyamsn97/Mario-GPT2-700-context-length model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianleguo/.pyenv/versions/3.8.18/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1833: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using shyamsn97/Mario-GPT2-700-context-length tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read)))\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "shape: torch.Size([1, 672]), torch.Size([1, 1401]) first: 56, last: 56: 100%|██████████| 1400/1400 [04:35<00:00,  5.09it/s]\n",
      "/Users/jianleguo/.pyenv/versions/3.8.18/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing level interactively -- /var/folders/p0/c1wj7grd6lb721vh0hbl8cq80000gn/T/tmpj9meckrq.txt!\n",
      "Running Astar agent on level! -- /var/folders/p0/c1wj7grd6lb721vh0hbl8cq80000gn/T/tmpyct9qr0c.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 685]), torch.Size([1, 2800]) first: 56, last: 13: 100%|██████████| 1400/1400 [06:04<00:00,  3.85it/s]\n",
      "/Users/jianleguo/.pyenv/versions/3.8.18/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing level interactively -- /var/folders/p0/c1wj7grd6lb721vh0hbl8cq80000gn/T/tmp_bet4kp1.txt!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mario_gpt import MarioLM, SampleOutput\n",
    "\n",
    "# pretrained_model = shyamsn97/Mario-GPT2-700-context-length\n",
    "\n",
    "mario_lm = MarioLM()\n",
    "\n",
    "# use cuda to speed stuff up\n",
    "# import torch\n",
    "# device = torch.device('cuda')\n",
    "# mario_lm = mario_lm.to(device)\n",
    "\n",
    "prompts = [\"many pipes, many enemies, some blocks, high elevation\"]\n",
    "\n",
    "# generate level of size 1400, pump temperature up to ~2.4 for more stochastic but playable levels\n",
    "generated_level = mario_lm.sample(\n",
    "    prompts=prompts,\n",
    "    num_steps=1400,\n",
    "    temperature=2.0,\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "# show string list\n",
    "generated_level.level\n",
    "\n",
    "# show PIL image\n",
    "generated_level.img\n",
    "\n",
    "# save image\n",
    "generated_level.img.save(\"generated_level.png\")\n",
    "\n",
    "# save text level to file\n",
    "generated_level.save(\"generated_level.txt\")\n",
    "\n",
    "# play in interactive\n",
    "generated_level.play()\n",
    "\n",
    "# run Astar agent\n",
    "generated_level.run_astar()\n",
    "\n",
    "# Continue generation\n",
    "generated_level_continued = mario_lm.sample(\n",
    "    seed=generated_level,\n",
    "    prompts=prompts,\n",
    "    num_steps=1400,\n",
    "    temperature=2.0,\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "# load from text file\n",
    "loaded_level = SampleOutput.load(\"generated_level.txt\")\n",
    "\n",
    "# play from loaded (should be the same level that we generated)\n",
    "loaded_level.play()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing level interactively -- /var/folders/p0/c1wj7grd6lb721vh0hbl8cq80000gn/T/tmpx0es05rc.txt!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-03-06 16:33:11.210 java[46981:1582020] TSM AdjustCapsLockLEDForKeyTransitionHandling - _ISSetPhysicalKeyboardCapsLockLED Inhibit\n"
     ]
    }
   ],
   "source": [
    "# play from loaded (should be the same level that we generated)\n",
    "loaded_level.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
